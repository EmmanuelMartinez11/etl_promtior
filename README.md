# Electric Vehicle Data Pipeline

Este repositorio contiene un pipeline de datos para ingestar, limpiar, transformar y almacenar datos de registros de vehÃ­culos elÃ©ctricos (Electric Vehicle Population Data) en PostgreSQL, orquestado con Apache Airflow.

---

## ğŸ“‹ DescripciÃ³n del Proyecto

El pipeline automatiza las siguientes etapas:

1. **Descarga automÃ¡tica** del CSV de datos con Beautiful Soup.
2. **Limpieza** del dataset usando Pandas (tipos de datos, valores nulos, normalizaciÃ³n de texto).
3. **ConversiÃ³n a Parquet** para optimizar lectura/escritura.
4. **CreaciÃ³n de esquema** en PostgreSQL (dimensiones y tabla de hechos).
5. **Carga masiva** de datos desde Parquet en las tablas usando SQLAlchemy y `COPY`.

El modelo de datos estÃ¡ diseÃ±ado en **star schema** con tablas de dimensiones (`dim_vehicle`, `dim_date`, `dim_location`, `dim_electric_type`, `dim_policy`) y la tabla de hechos `fact_registration`.

---

## ğŸš€ TecnologÃ­as y Herramientas

* Python 3.8+
* Pandas, Beautiful Soup
* Parquet (PyArrow)
* PostgreSQL 13
* SQLAlchemy
* Apache Airflow 2.x
* Docker & Docker Compose

---

## ğŸ“¦ Estructura del Repositorio

```
â”œâ”€â”€ dags/                 # DAG de Airflow
â”‚   â””â”€â”€ electric_vehicles_etl.py
â”œâ”€â”€ etl/                  # MÃ³dulos de Python
â”‚   â”œâ”€â”€ db_connection.py
â”‚   â”œâ”€â”€ data_reader.py
â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”œâ”€â”€ schema_creator.py
â”‚   â””â”€â”€ load_data.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.conf      # ParÃ¡metros (URL, credenciales)
â”œâ”€â”€ data/                # Archivos temporales (CSV, Parquet)
â”œâ”€â”€ docker-compose.yml   # OrquestaciÃ³n de Airflow y PostgreSQL
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md            # Esta documentaciÃ³n
```

---

## ğŸ› ï¸ Requisitos Previos

* Docker y Docker Compose instalados
* Puerto `8080` libre en `localhost` (para Airflow UI)
* Puerto `6060` libre en `localhost` (para PgAdmin)
* Puerto `5432` libre en `localhost` (para PostgreSQL)

---

## âš™ï¸ CÃ³mo ejecutar

1. **Clonar el repositorio**:

   ```bash
   git clone https://github.com/EmmanuelMartinez11/etl_promtior
   ```

2. **Revisar configuraciÃ³n** (opcional):
   * Esta configuraciÃ³n tiene como porposito el facilitar las ejecuciones en la base de datos levantada en docker, por lo que se recomienda modificar las credenciales tanto de docker-compose.yml como de config.conf

3. **Levantar servicios**:

   ```bash
   docker-compose up -d
   ```

   * Esto inicia PostgreSQL en `localhost:5432`,  PgAdmin en `http://localhost:8080` y Airflow en `http://localhost:8080`.

4. **Configurar Airflow**:

   * Usuario: `admin` | ContraseÃ±a: `admin`
   * Activa el DAG `electric_vehicles_etl` y ejecuta manualmente la primera ejecuciÃ³n.

5. **Verificar carga de datos**:
  Opcion 1:
   * ConÃ©ctate a PostgreSQL (`localhost:5432`, usuario `airflow`, DB `promtior_etl`).
   * Revisa las tablas: `dim_*` y `fact_registration`.
  Opcion 2:
   * ConÃ©ctate a PgAdmin (`localhost:6060`, usuario `admin@admin.com`, DB `promtior_etl`).
   * Revisa las tablas: `dim_*` y `fact_registration`. usando las consultas SELECT
